{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KGT-ActiveLearning.ipynb CLT8ZP",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balazsmorv/AppleMusicKit/blob/main/KGT_ActiveLearning_ipynb_CLT8ZP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsKlJD9CXBGU"
      },
      "source": [
        "# Aktív tanulás\n",
        "\n",
        "A múlt félév végén megvizsgáltuk a $K$-karú rabló-problémát, mint sok MI/megerősítéses tanulási/szekvenciális döntési feladat formális alapját. Ezt a keretet az orvosbiológiai kísérlettervezéstől kezdve a deep learning-alapú Go-játékos gépekig számos területen sikeresen alkalmazták; ezen a gyakorlaton egyszerűbb ágenseket fogunk implementálni az alapfeladat megoldására (néhányról volt szó az előadáson is).\n",
        "\n",
        "## Emlékeztető\n",
        "\n",
        "A félkarú rabló mintájára legyen adott $K$-karú játékgép, ahol, ha szerencsénk van, egy kart meghúzva valamekkora jutalomban (*reward*) részesülhetünk. Az egyes karok által adott jutalmak valószínűségi változók, amelyek ismeretlen, egymástól független eloszlásokat követnek. A célunk, hogy a karok egymás után történő húzogatásával\n",
        "- Felfedezzük a problémateret, azaz megbecsüljük az egyes karok \"jóságát\" (*exploration*),\n",
        "- Ezt a tudást kihasználva maximalizáljuk az összes jutalmunkat (*exploitation*).\n",
        "\n",
        "Érezzük, hogy a két cél egymásnak ellentmond: a felfedezéshez szuboptimális karokat is meg kell húznunk, ezzel azonban az összes jutalmunkat csökkentjük. A karok valószínűségi természete miatt az is előfordulhat, hogy egy-egy \"jó\" kar éppen rossz jutalmat ad, így érdemes lehet a pillanatnyilag rossznak gondolt karokat is többször tesztelni. Ez az *exploration vs. exploitation* dilemma.\n",
        "\n",
        "Bár az általunk vizsgált stratégiák többféle eloszlás esetében is működnek, mi a már jól ismert Bernoulli-eloszlást fogjuk használni. Azaz minden karhoz tartozik egy ismeretlen $\\theta_k$ paraméter, egy húzás pedig egy Bernoulli-kísérletnek felel meg:\n",
        "\n",
        "\\begin{align}\n",
        "p\\left(x \\mid \\theta_k \\right) = \\mathcal{B}ern \\left(x \\mid \\theta_k \\right) = \\theta_k^{x} (1-\\theta_k)^{1-x},\n",
        "\\end{align}\n",
        "\n",
        "ahol $x \\in \\left\\lbrace 0,1 \\right\\rbrace$ a $k$. kar meghúzása után kapott jutalom. A továbbiakban jelölje $\\bar x_k$ a $k.$ kar által adott jutalmak átlagát, $n_k$ azt, hogy hányszor húztuk meg a $k$. kart, $n$ pedig az összes húzások számát egészen mostanáig.\n",
        "\n",
        "Egy stratégia fontos jellemzője a megbánás (*regret*), ami a mi esetünkben a következőképpen definiálható:\n",
        "\n",
        "\\begin{align}\n",
        "\\sum_{k=1}^K n_k \\left(\\theta^* - \\theta_k \\right), \n",
        "\\end{align}\n",
        "\n",
        "ahol $\\theta^*$ a legjobb kar jutalmának várható értéke, $\\theta_k$ pedig a $k$. karé (ami most, Bernoulli-eloszlásról lévén szó, megegyezik a paraméterrel). Azaz itt valójában azt mérjük, hogy várhatóan mennyit veszítünk amiatt, hogy a stratégiánk nem mindig a legjobb kart választja.\n",
        "\n",
        "## Stratégiák\n",
        "\n",
        "A következő stratégiákat fogjuk implementálni (az első kettőt láttuk előadáson):\n",
        "\n",
        "- $\\varepsilon$-greedy: adott $\\varepsilon$ valószínűséggel felfedezünk, azaz véletlenszerűen válaszott kart húzunk meg; különben azt, amelyik eddig átlagosan a legnagyobb kifizetést adta (azaz $\\bar x_k$ maximális).\n",
        "- UCB1: először minden kart meghúzunk egyszer, majd mindig azt húzzuk, amelyik a $\\bar x_k + \\sqrt{\\frac{2 \\ln n}{n_k}}$ mennyiséget maximalizálja.\n",
        "- UCB-$\\alpha$: hasonlóan az előbbihez, de $\\bar x_k + \\sqrt{\\frac{\\alpha \\ln n}{n_k}}$-t használunk.\n",
        "- Thompson-mintavétel: minden karra felteszünk egy $\\beta$ prior eloszlást, majd mintát veszünk és a maximálisat választjuk; a jutalom ismeretében a $\\beta$-eloszlás frissítése a már ismert módon történik (konjugált eloszlások, tavalyi első labor).\n",
        "- Bayesi UCB: hasonlóképpen fenntartunk egy $\\beta$-eloszlást, de a választásnál nem mintát veszünk, hanem $1- \\frac{1}{1+n}$ percentilist veszünk alapul (pl. a `beta.ppf()` függvénnyel).\n",
        "\n",
        "## Feladatok\n",
        "\n",
        "**1. feladat.** Valósítsa meg az ágenseket a lenti prototípus alapján! Az ágenseknek a `get_arm()` és az `update()` függvényeket kötelező implementálniuk.\n",
        "\n",
        "A kiértékelést végezze el mind a 3 megadott paraméterezéssel. Ügyeljen rá, hogy az eredmények az egyes futásoknál eltérhetnek, így célszerű többször is lefuttatni a méréseket. Különösen az $\\varepsilon$-greedy stratégia mutat nagy variabilitást; magyarázza meg, hogy miért!\n",
        "\n",
        "**2. feladat.** Minden ágenshez ábrázolja scatter plot-on, mikor melyik kart húzta meg (vízszintes tengely: húzások, függőleges tengely: karok)! A plotok alapján hasonlítsa össze az ágensek felfedező tevékenységét!\n",
        "\n",
        "**3. feladat.** Értékelje ki és hasonlítsa össze az ágensek teljesítményét! A következőket vegye alapul:\n",
        "\n",
        "- Átlagos jutalom\n",
        "- A legjobb kar meghúzásának relatív frekvenciája\n",
        "- Regret\n",
        "\n",
        "Az eredményeket az idő (húzások) függvényében is ábrázolja (azaz az ágens tároljon el minden ehhez szükséges információt)! Miben különbözik az $\\varepsilon$-greedy ágens regret-je a többi módszerétől?\n",
        "\n",
        "**4. feladat.** Tetszőleges problémában ábrázolja a bayesi UCB ágens $\\beta$-posteriorjait a tanítás legvégén! Mit lehet megfigyelni? Miért mondhatjuk, hogy ez egy \"optimista\" stratégia?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g412dwd0meAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f0ef5b-4cfe-45d3-9f17-8fad21cb8e01"
      },
      "source": [
        "%pylab inline\n",
        "from scipy.stats import beta\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['beta']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI9z8jhzml8F"
      },
      "source": [
        "# Prototípus ágens (példa)\n",
        "class Agent:\n",
        "  def __init__(self,K):\n",
        "    self.X = [] # eddigi jutalmak\n",
        "    self.A = [] # melyik kart húztuk\n",
        "    self.K = K  # karok száma\n",
        "\n",
        "  # Kötelező: melyik legyen a következő kar\n",
        "  def get_arm(self):\n",
        "    self.arm = np.random.choice(self.K)\n",
        "    return self.arm\n",
        "\n",
        "  # Kötelező: az ágens állapotának frissítése\n",
        "  def update(self,reward):\n",
        "    self.A += [self.arm]\n",
        "    self.X += [reward]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKU58NAlU_EF"
      },
      "source": [
        "# 1. feladat megoldása (ágensek létrehozása)\n",
        "# Prototípus ágens (példa)\n",
        "class EGreedy:\n",
        "  def __init__(self,K,e):\n",
        "    self.X = [] # eddigi jutalmak\n",
        "    self.A = [] # melyik kart húztuk\n",
        "    self.K = K  # karok száma\n",
        "\n",
        "    self.e = e # felfedezés valsége\n",
        "    self.y = np.zeros(K) # az egyes karok eddigi jutalma\n",
        "    self.n = np.ones(K) # az egyes karok meghúzása, laplace simítás\n",
        "\n",
        "  # Kötelező: melyik legyen a következő kar\n",
        "  def get_arm(self):\n",
        "    if np.random.rand() < self.e:\n",
        "      self.arm = np.random.choice(self.K)\n",
        "    else:\n",
        "      self.arm = np.argmax(self.y / self.n)\n",
        "    return self.arm\n",
        "\n",
        "  # Kötelező: az ágens állapotának frissítése\n",
        "  def update(self,reward):\n",
        "    self.A += [self.arm]\n",
        "    self.X += [reward]\n",
        "    self.y[self.arm] += reward\n",
        "    self.n[self.arm] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UCBA:\n",
        "  def __init__(self,K, alpha):\n",
        "    self.X = [] # eddigi jutalmak\n",
        "    self.A = [] # melyik kart húztuk\n",
        "    self.K = K  # karok száma\n",
        "    self.alpha = alpha # alpha = 2 esetén UCB1\n",
        "    self.y = np.zeros(K) # az egyes karok eddigi jutalma\n",
        "    self.n = np.ones(K) # az egyes karok meghúzása, laplace simítás\n",
        "\n",
        "  def value_for_arm(self, arm_index):\n",
        "    mean_xk = float(self.y[arm_index] / self.n[arm_index])\n",
        "    return mean_xk + np.sqrt((self.alpha * np.log(sum(self.n))) / self.n[arm_index])\n",
        "\n",
        "  # Kötelező: melyik legyen a következő kar\n",
        "  def get_arm(self):\n",
        "    # ameddig nem húztuk meg minden kart addig mindig másikat húzunk\n",
        "    not_yet_pulled_indices = np.flatnonzero(self.n < 2)\n",
        "    if len(not_yet_pulled_indices) > 0:\n",
        "      self.arm = np.random.choice(not_yet_pulled_indices)\n",
        "    else:\n",
        "      values = []\n",
        "      for i in range(0, self.K):\n",
        "        values.append(self.value_for_arm(i))\n",
        "      self.arm = np.argmax(values)\n",
        "    return self.arm\n",
        "\n",
        "  # Kötelező: az ágens állapotának frissítése\n",
        "  def update(self,reward):\n",
        "    self.A += [self.arm]\n",
        "    self.X += [reward]\n",
        "    self.y[self.arm] += reward\n",
        "    self.n[self.arm] += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "I0Asg8SQjf5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Thomson:\n",
        "  def __init__(self, K, prior_a, prior_b):\n",
        "    self.X = [] # eddigi jutalmak\n",
        "    self.A = [] # melyik kart húztuk\n",
        "    self.K = K  # karok száma\n",
        "\n",
        "    self.prior_alphas = prior_a\n",
        "    self.prior_betas = prior_b\n",
        "\n",
        "    self.y = np.zeros(K) # az egyes karok eddigi jutalma\n",
        "    self.n = np.ones(K) # az egyes karok meghúzása, laplace simítás\n",
        "\n",
        "  # Kötelező: melyik legyen a következő kar\n",
        "  def get_arm(self):\n",
        "    # mindegyik kar priorjából mintát veszünk, legnagyobbat választjuk\n",
        "    samples = []\n",
        "    for i in range(0, self.K):\n",
        "      samples.append(np.random.beta(self.prior_alphas[i], self.prior_betas[i]))\n",
        "    self.arm = np.argmax(samples)\n",
        "    return self.arm\n",
        "\n",
        "  # Kötelező: az ágens állapotának frissítése\n",
        "  def update(self,reward):\n",
        "    self.A += [self.arm]\n",
        "    self.X += [reward]\n",
        "    self.y[self.arm] += reward\n",
        "    self.n[self.arm] += 1\n",
        "    if reward == 1:\n",
        "      self.prior_alphas[self.arm] += 1\n",
        "    else:\n",
        "      self.prior_betas[self.arm] += 1"
      ],
      "metadata": {
        "id": "fy-BPDlvv-jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianUCB:\n",
        "  def __init__(self, K, prior_a, prior_b):\n",
        "    self.X = [] # eddigi jutalmak\n",
        "    self.A = [] # melyik kart húztuk\n",
        "    self.K = K  # karok száma\n",
        "\n",
        "    self.prior_alphas = prior_a\n",
        "    self.prior_betas = prior_b\n",
        "\n",
        "    self.y = np.zeros(K) # az egyes karok eddigi jutalma\n",
        "    self.n = np.ones(K) # az egyes karok meghúzása, laplace simítás\n",
        "\n",
        "  # Kötelező: melyik legyen a következő kar\n",
        "  def get_arm(self):\n",
        "    # 1 - (1 / (1+n)) percentilist számolunk\n",
        "    self.arm = np.argmax(beta.ppf(1 - (1/(1+sum(self.n))), self.prior_alphas, self.prior_betas))\n",
        "    return self.arm\n",
        "\n",
        "  # Kötelező: az ágens állapotának frissítése\n",
        "  def update(self,reward):\n",
        "    self.A += [self.arm]\n",
        "    self.X += [reward]\n",
        "    self.y[self.arm] += reward\n",
        "    self.n[self.arm] += 1\n",
        "    if reward == 1:\n",
        "      self.prior_alphas[self.arm] += 1\n",
        "    else:\n",
        "      self.prior_betas[self.arm] += 1"
      ],
      "metadata": {
        "id": "JWZC29_51in5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnuIvdtrn9KV"
      },
      "source": [
        "# Szimuláció T db húzással\n",
        "def simulate(rewards,agent,T=10000):\n",
        "  for t in range(T):\n",
        "    arm    = agent.get_arm()\n",
        "    reward = (np.random.rand()<rewards[arm])*1\n",
        "    agent.update(reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1deq0ZHoGsR"
      },
      "source": [
        "#reward_probs = np.linspace(0.01,0.99,30)\n",
        "#reward_probs = [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.795, 0.8, 0.805]\n",
        "reward_probs = [0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wOo3CF3KhdX"
      },
      "source": [
        "# 1. feladat megoldása (szimuláció) EGreedy\n",
        "\n",
        "e_agent = EGreedy(len(reward_probs), 0.1)\n",
        "simulate(reward_probs, e_agent)\n",
        "print(e_agent.n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Az epsilon-greedy stratégia nagy variabilitást mutat, mivel minden húzásnál 1-e valséggel (nagy) a legnagyobb átlagos kifizetésű kart húzzuk, ami messze lehet az optimálistól, hiszen kis valséggel húzzuk meg a legjobb kart, illetve ha meghúzzuk, akkor se biztos hogy elégszer ahhoz, hogy az átlag kifizetése a legjobb legyen."
      ],
      "metadata": {
        "id": "VHzowSXn6Vgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. feladat megoldása (szimuláció) UCB1 és UCBA\n",
        "\n",
        "ucb1_agent = UCBA(len(reward_probs), 2.0)\n",
        "simulate(reward_probs, ucb1_agent)\n",
        "print(ucb1_agent.n)\n",
        "\n",
        "ucbA_agent = UCBA(len(reward_probs), 5.0)\n",
        "simulate(reward_probs, ucbA_agent)\n",
        "print(ucbA_agent.n)"
      ],
      "metadata": {
        "id": "qiH8CYQv0mUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. feladat megoldása (szimuláció) Thomson és Bayesi UCB\n",
        "\n",
        "priorA = np.ones(len(reward_probs))\n",
        "priorB = np.ones(len(reward_probs))\n",
        "\n",
        "thomson_agent = Thomson(len(reward_probs), priorA, priorB)\n",
        "simulate(reward_probs, thomson_agent)\n",
        "print(thomson_agent.n)\n",
        "\n",
        "bayesianUCB_agent = BayesianUCB(len(reward_probs), priorA, priorB)\n",
        "simulate(reward_probs, bayesianUCB_agent)\n",
        "print(bayesianUCB_agent.n)"
      ],
      "metadata": {
        "id": "uzW9CHdm0uwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****2. feladat****"
      ],
      "metadata": {
        "id": "mx4m8Lob71Nx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZJ2GJo8qWWP"
      },
      "source": [
        "# 2. feladat megoldása\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r2_ZQoQqeMf"
      },
      "source": [
        "# 3. feladat megoldása\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHXQv5n5qd0I"
      },
      "source": [
        "# 4. feladat megoldása\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}